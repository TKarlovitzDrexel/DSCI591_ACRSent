{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)\n",
    "\n",
    "#display max columns and rows\n",
    "pd.options.display.max_rows\n",
    "pd.options.display.max_columns\n",
    "\n",
    "# For regular expressions\n",
    "import re\n",
    "# For handling string\n",
    "import string\n",
    "# For performing mathematical operations\n",
    "import math\n",
    "\n",
    "#text preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "tokenizer = ToktokTokenizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "for f in files:\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data\n",
    "el = pd.read_csv(\"/Users/dustin.ellis/Desktop/Desktop/Drexel University Data Science/Classes/DSCI_591-Capstone_1/Exploratory Data Analysis/amazon_reviews_us_Electronics_v1_00.tsv\",sep=\"\\t\", error_bad_lines = False)\n",
    "el.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the information about the dataset\n",
    "el.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get basic stats on numerical columns. Customer ID and product parent should be treated as objects. \n",
    "el.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get counts of star ratings to get a sense of how many reviews there are\n",
    "el['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el['helpful_votes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el['total_votes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram for each numerical attribute to get a feel of data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "el.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section is the data cleaning. In the cleaning process, the vine column was removed as it did not contain useful information. A year column was created, as specific date was not of interest and year is typically used to examine trends in technology. Having the date column would therefore be redundant. I wanted to ensure that only products that were verified purchases by Amazon and sold in the US marketplace were used, so any of the products that did not meet this criteria were dropped. Rows containing null values were dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop vine column because it is useless:\n",
    "el = el.drop(['vine'], axis = 1)\n",
    "\n",
    "#extract year from review date column\n",
    "el['year'] = pd.DatetimeIndex(el['review_date']).year\n",
    "\n",
    "#drop review date column since year is extracted\n",
    "el = el.drop(['review_date'], axis = 1)\n",
    "\n",
    "#count the # of reviews per year\n",
    "el['year'].value_counts(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el['helpful_votes'].value_counts()\n",
    "#if you run, you will see that many people do not leave helpful vote clicks on reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el['review_headline'].value_counts()\n",
    "#I was curious to see if there were common headlines people write when reviewing. It appears that commonly, people will write out a star review and then elaborate on it rather than make a direct comment about something in their review to preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate rows where verified purchase = N and where marketplace != US\n",
    "el.drop(el.loc[el['verified_purchase']=='N'].index, inplace=True)\n",
    "el.drop(el.loc[el['marketplace']!='US'].index, inplace=True)\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nulls\n",
    "el.isnull().sum()\n",
    "#drop nulls\n",
    "el.dropna(inplace=True)\n",
    "el.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Create a generic Title per Amazons guidelines) is a null value for product title that was discovered. It is important to remove these values, as a blank product title could be anything and we only want to include products that have a clear product title.\n",
    "el = el[el.product_title   != \"(Create a generic Title per Amazons guidelines)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sense of the unique products in the dataset. There are products such as mounts, cases, sleeves, cleaners, cables, etc. that are not electronic devices themselves. I decided to remove these products as reviews for these kinds of products are going to widely differ from reviews for actual electronic devices.\n",
    "#el['product_title'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next segment is pre-processing. The dataframe was converted into lowercase text to make it easier to work with. Next, only products that had 500+ reviews were selected, yielding a total of 679 unique products. Products that were mounts, cases, sleeves, or otherwise products sold in electronics departments that are not devices themselves were removed. Lastly, 60% of the data was randomly selected to work with to make analysis feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to only take products with 500 reviews or more\n",
    "el=el.groupby('product_title').filter(lambda x:len(x)>500).reset_index(drop=True)\n",
    "print('Number of products=>',len(el['product_title'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase the dataframe\n",
    "el = el.apply(lambda x: x.astype(str).str.lower())\n",
    "el.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows where product is not necessarily an electronic device\n",
    "el = el[~el['product_title'].str.contains('mount|splitter|hdmi|case|cable|sleeve|adapter|famale|female|male|charger|chord|amazonbasics|spray|cleaner|wipes|batteries')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el['product_title'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataframe in a particular size\n",
    "el = el.sample(frac=0.6,random_state=200)\n",
    "el.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at some product reviews to get a sense of what they are like\n",
    "#for index,text in enumerate(el['review_body'][35:40]):\n",
    "  #print('Review %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak at unique products that remaine after cleaning. \n",
    "#el['product_title'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next segment is where exploratory data analysis and feature engineering were conducted. The number of reviews for the top and bottom 20 products was calculated and plotted. Next, review bodies were cleaned to remove punctuations and contractions. This was done as to be able to accurately analyze polarity, helpfulness ratio, frequency of words within reviews, etc. WordClouds were generated for each product to understand which words appear most frequently in review for each product. Other graphs examined the helpfulness of reviews in regard to star ratings and review text length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine value counts of the top 20 reviewed products after the junk products have been filtered out\n",
    "\n",
    "products = el[\"product_title\"].value_counts()\n",
    "plt.figure(figsize=(12,8))\n",
    "products[:20].plot(kind='bar')\n",
    "plt.title(\"Number of Reviews for Top 20 Products\")\n",
    "plt.xlabel('Product Title')\n",
    "plt.ylabel('Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine value counts of the bottom 20 reviewed products after the junk products have been filtered out\n",
    "\n",
    "products = el[\"product_title\"].value_counts()\n",
    "# brands.count()\n",
    "plt.figure(figsize=(12,8))\n",
    "products[-20:].plot(kind='bar')\n",
    "plt.title(\"Number of Reviews for Bottom 20 Products\")\n",
    "plt.xlabel('Product Title')\n",
    "plt.ylabel('Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "el['review_body']=el['review_body'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase review bodies\n",
    "el['cleaned_review_bodies']=el['review_body'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digits and words containing digits\n",
    "el['cleaned_revs'] = el['cleaned_review_bodies'].apply(lambda x: re.sub('\\w*\\d\\w*','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "el['cleaned_revs']=  el['cleaned_review_bodies'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extra spaces\n",
    "el['cleaned_revs']=el['cleaned_revs'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview of cleaned review body text\n",
    "#for index,text in enumerate(el['cleaned_revs'][35:40]):\n",
    "  #print('Review %d:\\n'%(index+1),text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation of spaCy to access language analytical tools \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing spacy\n",
    "#import spacy\n",
    "\n",
    "# Loading model\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatization with stopwords removal\n",
    "el['lemmatized']=el['cleaned_revs'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group reviews according to products\n",
    "\n",
    "el_grouped=el[['product_title','lemmatized']].groupby(by='product_title').agg(lambda x:' '.join(x))\n",
    "el_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document Term Matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(analyzer='word')\n",
    "data=cv.fit_transform(el_grouped['lemmatized'])\n",
    "el_dtm = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\n",
    "el_dtm.index=el_grouped.index\n",
    "el_dtm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textwrap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "\n",
    "# Function for generating word clouds\n",
    "def generate_wordcloud(data,title):\n",
    "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\").generate_from_frequencies(data)\n",
    "  plt.figure(figsize=(10,8))\n",
    "  plt.imshow(wc, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.title('\\n'.join(wrap(title,60)),fontsize=13)\n",
    "  plt.show()\n",
    "  \n",
    "# Transposing document term matrix\n",
    "el_dtm=el_dtm.transpose()\n",
    "\n",
    "# Plotting word cloud for each product. Run if you would like to see these!\n",
    "for index,product in enumerate(el_dtm.columns):\n",
    "    generate_wordcloud(el_dtm[product].sort_values(ascending=False),product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "el['polarity']= el['lemmatized'].apply(lambda x:TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3 Random Reviews with Highest Polarity:\")\n",
    "for index,review in enumerate(el.loc[el['polarity'].sort_values(ascending=False)[:3].index]['review_body']):\n",
    "    print('Review {}:\\n'.format(index+1),review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3 Random Reviews with Lowest Polarity:\")\n",
    "for index,review in enumerate(el.loc[el['polarity'].sort_values(ascending=True)[:3].index]['review_body']):\n",
    "  print('Review {}:\\n'.format(index+1),review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity plot of 20 products with lowest polarity\n",
    "product_polarity_sorted=pd.DataFrame(el.groupby('product_title')['polarity'].mean().sort_values(ascending=True)[:20])\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Products')\n",
    "plt.title('Polarity of Different Amazon Product Reviews')\n",
    "polarity_graph=plt.barh(np.arange(len(product_polarity_sorted.index)),product_polarity_sorted['polarity'],color='blue',)\n",
    "\n",
    "# Writing product names on bar\n",
    "for bar,product in zip(polarity_graph,product_polarity_sorted.index):\n",
    "  plt.text(0.005,bar.get_y()+bar.get_width(),'{}'.format(product),va='center',fontsize=11,color='white')\n",
    "\n",
    "# Writing polarity values on graph\n",
    "for bar,polarity in zip(polarity_graph,product_polarity_sorted['polarity']):\n",
    "  plt.text(bar.get_width()+0.001,bar.get_y()+bar.get_width(),'%.3f'%polarity,va='center',fontsize=11,color='black')\n",
    "  \n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity plot of 20 products with highest polarity\n",
    "product_polarity_sorted=pd.DataFrame(el.groupby('product_title')['polarity'].mean().sort_values(ascending=False)[:20])\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Products')\n",
    "plt.title('Polarity of Different Amazon Product Reviews')\n",
    "polarity_graph=plt.barh(np.arange(len(product_polarity_sorted.index)),product_polarity_sorted['polarity'],color='orange',)\n",
    "\n",
    "# Writing product names on bar\n",
    "for bar,product in zip(polarity_graph,product_polarity_sorted.index):\n",
    "  plt.text(0.005,bar.get_y()+bar.get_width(),'{}'.format(product),va='center',fontsize=11,color='white')\n",
    "\n",
    "# Writing polarity values on graph\n",
    "for bar,polarity in zip(polarity_graph,product_polarity_sorted['polarity']):\n",
    "  plt.text(bar.get_width()+0.001,bar.get_y()+bar.get_width(),'%.3f'%polarity,va='center',fontsize=11,color='black')\n",
    "  \n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are able to get reading levels for the reviews. A reason why you may want to do this is to see whether people write more complex reviews for 1 star vs. 5 star reviews. \n",
    "\n",
    "The Dale–Chall readability formula is a readability test that provides a numeric gauge of the comprehension difficulty that readers come upon when reading a text. It uses a list of 3000 words that groups of fourth-grade American students could reliably understand, considering any word not on that list to be difficult.\n",
    "\n",
    "\n",
    "The Flesch–Kincaid readability tests are readability tests designed to indicate how difficult a passage in English is to understand. The Flesch Reading-Ease uses word length and sentence length are used along with weighting factors to determine reading ease.In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. \n",
    "\n",
    "The Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior (around 18 years old).\n",
    "\n",
    "To do this analysis, you have to import textstat.Textstat is an easy to use library to calculate statistics from text. It helps determine readability, complexity, and grade level.\n",
    "\n",
    "Running this analysis is really time consuming. This is probably best to do with small chunks of the data, rather than a large dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import textstat\n",
    "\n",
    "#el['dale_chall_score']=el['review_body'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "#el['flesh_reading_ease']=el['review_body'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "#el['gunning_fog']=el['review_body'].apply(lambda x: textstat.gunning_fog(x))\n",
    "\n",
    "#print('Dale Chall Score of upvoted reviews=>',el[el['helpful_votes']>1]['dale_chall_score'].mean())\n",
    "#print('Dale Chall Score of not upvoted reviews=>',el[el['helpful_votes']<=1]['dale_chall_score'].mean())\n",
    "\n",
    "#print('Flesch Reading Score of upvoted reviews=>',el[el['helpful_votes']>1]['flesh_reading_ease'].mean())\n",
    "#print('Flesch Reading Score of not upvoted reviews=>',el[el['helpful_votes']<=1]['flesh_reading_ease'].mean())\n",
    "\n",
    "#print('Gunning Fog Index of upvoted reviews=>',el[el['helpful_votes']>1]['gunning_fog'].mean())\n",
    "#print('Gunning Fog Index of not upvoted reviews=>',el[el['helpful_votes']<=1]['gunning_fog'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert star rating, helpful votes, and total votes from object to integer to allow for caluclations and manipulations to be performed. \n",
    "el[\"star_rating\"] = el[\"star_rating\"].astype(str).astype(int)\n",
    "el[\"helpful_votes\"] = el[\"helpful_votes\"].astype(str).astype(int)\n",
    "el[\"total_votes\"] = el[\"total_votes\"].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_1 = el[(el['product_title']==\"beats solo hd over-ear headphone\") & (el['star_rating']>3)]\n",
    "j_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j_1['cleaned_review_bodies'][381279])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "def RegExpTokenizer(Sent):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(Sent)\n",
    "\n",
    "ListWords1 = []\n",
    "for m in j_1['cleaned_review_bodies']:\n",
    "    n = RegExpTokenizer(str(m))\n",
    "    ListWords1.append(n)\n",
    "print(ListWords1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Words\n",
    "from nltk import FreqDist\n",
    "def Bag_Of_Words(ListWords1):\n",
    "    all_words1 = []\n",
    "    for m in ListWords1:\n",
    "        for w in m:\n",
    "            all_words1.append(w.lower())\n",
    "    all_words2 = FreqDist(all_words1)\n",
    "    return all_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "all_words3 = Bag_Of_Words(ListWords1)\n",
    "ax = plt.figure(figsize=(15,10))\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color='white',max_font_size=40).generate(' '.join(all_words3.keys()))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(\"Combien de Mots !!!\",len(all_words3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "all_words5 = Bag_Of_Words(ListWords1)\n",
    "count = []\n",
    "Words  = []\n",
    "for w in all_words5.most_common(10):\n",
    "    count.append(w[1])\n",
    "    Words.append(w[0])\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.barplot(Words,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total numbers of ratings in the home and kitchen product reviews\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.countplot(el['star_rating'])\n",
    "plt.title('Total Review Numbers for Each Rating', color='r')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Customer totals for each rating class\n",
    "el['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "el.groupby('star_rating').star_rating.count()\n",
    "el.groupby('star_rating').star_rating.count().plot(kind='pie',autopct='%1.1f%%',startangle=90,explode=(0,0.1,0,0,0),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data= el.copy()\n",
    "word_count=[]\n",
    "for s1 in el.cleaned_review_bodies:\n",
    "    word_count.append(len(str(s1).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(x=\"star_rating\",y=word_count,data=el)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Review Length')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there are outliers in the above boxplot we are not able to clearly visualize.So remove the outliers \n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "sns.boxplot(x=\"star_rating\",y=word_count,data=el,showfliers=False)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Review Length')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total review for every year for the beats solo hd over-ear headphone\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.countplot(el['year'])\n",
    "plt.title('Total Review Numbers for Each Year', color='r')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Customer totals for each rating class\n",
    "el['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be of interest to understand the quantity of unique customers in the dataset. Do we see more customers year after year? How many customers leave 1-star vs 5-star ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique customers do we have in the dataset?\n",
    "print('Number of unique customers: {}'.format(len(el['customer_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique reviews do we have in the dataset?\n",
    "print('Number of unique reviews: {}'.format(len(el['cleaned_review_bodies'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique customers in each year?\n",
    "unique_cust = el.groupby('year')['customer_id'].nunique()\n",
    "\n",
    "# Plot unique customer numbers in each year\n",
    "plt.figure(figsize = (10,6))\n",
    "unique_cust.plot(kind='bar', rot = 0, color = 'blue')\n",
    "plt.title('Unique Customers in Each Year', color='gray', size = 14)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Unique Customer Numbers')\n",
    "plt.show()\n",
    "\n",
    "# Print unique customer numbers in each year\n",
    "print(unique_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique customers for each \"star_rating\"\n",
    "a = list(el.groupby(['star_rating'])['customer_id'].unique())  \n",
    "\n",
    "# number of customers\n",
    "a2 = [len(a[0]),len(a[1]), len(a[2]), len(a[3]), len(a[4])] \n",
    "\n",
    "# number of reviews for each \"star_rating\"\n",
    "b = list(el['star_rating'].value_counts())              \n",
    "\n",
    "\n",
    "\n",
    "uniq_cust_rate = pd.DataFrame({'star_rating': ['1', '2', '3', '4', '5'],\n",
    "                               'number_of_customers': a2,\n",
    "                               'number_of_reviews': sorted(b)})\n",
    "print(uniq_cust_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to understand how many unique products there are in the dataset. How many unique products are there each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique electronic products in the dataset\n",
    "print('Number of unique electronic products: {}'.format(len(el['product_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique electronic products in each year?\n",
    "unique_prod = el.groupby('year')['product_id'].nunique()\n",
    "\n",
    "# Plot unique product numbers in each year\n",
    "plt.figure(figsize = (10,6))\n",
    "unique_prod.plot(kind='bar', color = 'blue', rot =0)\n",
    "plt.title('Unique Products in Each Year', color = 'gray', size = 14)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Unique Product Numbers')\n",
    "plt.show()\n",
    "\n",
    "# Print unique product numbers in each year\n",
    "print(unique_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering: helpfulness ratio. Helpfulness ratio is calculated by taking the # of helpful votes and dividng it by the total votes. This tells you out of the total votes, which proportion of the votes were actually meaningful. This feature may differ across star ratings and across products. It may also have some impact on how reviews are structured. Let's find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el['helpfulness_proportion'] = el['helpful_votes']/el['total_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How helpful are star ratings?\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "el.groupby('star_rating')['helpfulness_proportion'].mean().plot(kind='bar', color=['black', 'red', 'green', 'blue', 'cyan']) \n",
    "plt.title(\"Helpfulness in rating\",color='r')\n",
    "plt.xlabel(\"Star Rating\")\n",
    "plt.ylabel(\"Helpfulness ratio\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of star rating in products\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "el1 = el.copy()\n",
    "el1 = el1[np.isfinite(el1['helpfulness_proportion'])]\n",
    "grp = el1.groupby('product_id')\n",
    "counts = grp.product_id.count()        # number of reviews by each critic\n",
    "means = grp.helpfulness_proportion.mean()     # average freshness for each critic\n",
    "\n",
    "means[counts > 5].hist(bins=10, edgecolor='w', lw=1)\n",
    "plt.xlabel(\"Average Helpfullness per product\")\n",
    "plt.ylabel(\"Number of products\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpfulness based on length of text\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "el1['review_length'] = el1.cleaned_review_bodies.apply(len)\n",
    "maxTextLen = max(el1.review_length)\n",
    "el1.groupby(pd.cut(el1['review_length'], np.arange(0,maxTextLen+1000,1000)))['helpfulness_proportion'].mean().plot(kind='bar',color='blue')\n",
    "plt.xlabel(\"length of review text\")\n",
    "plt.ylabel(\"Helpfulness ratio\")\n",
    "plt.title(\"Relationship between 'Helpfulness ratio' and 'Length of review text'\")\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How long are reviews typically and how many of them are there?\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "review_length = el[\"cleaned_review_bodies\"].dropna().map(lambda x: len(x))\n",
    "plt.figure(figsize=(12,8))\n",
    "review_length.loc[review_length < 2000].hist()\n",
    "plt.title(\"Distribution of Review Length\")\n",
    "plt.xlabel('Review length')\n",
    "plt.ylabel('Number of Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check helpfulness ratio below 2000 words\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "el1['review_length'] = el1.cleaned_review_bodies.apply(len)\n",
    "maxTextLen = 2000\n",
    "el1.groupby(pd.cut(el1['review_length'], np.arange(0,maxTextLen,100)))['helpfulness_proportion'].mean().plot(kind='bar',color='blue')\n",
    "plt.xlabel(\"length of review text\")\n",
    "plt.ylabel(\"Helpfulness ratio\")\n",
    "plt.title(\"Relationship between 'Helpfulness ratio' and 'Length of review text < 2000'\")\n",
    "plt.ylim([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label helpfulness\n",
    "def label_Helpfulness(row):\n",
    "    if row['helpfulness_proportion'] > 0.75:\n",
    "           return 'helpful'\n",
    "    else: \n",
    "           return 'unhelpful'\n",
    "\n",
    "el1['helpIndx'] = el1.apply(label_Helpfulness, axis=1)\n",
    "\n",
    "el1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text length\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "subset_el1 = el1[(el1.star_rating>=2) & (el1.review_length>=2)]\n",
    "\n",
    "subset_el1.review_length[subset_el1.review_length<2000].plot(bins=50, kind='hist',alpha=0.2,color='blue')\n",
    "dhelp = subset_el1.review_length[(subset_el1['helpIndx']=='helpful') & (subset_el1.review_length<2000)]\n",
    "dunhelp = subset_el1.review_length[(subset_el1['helpIndx']=='unhelpful') & (subset_el1.review_length<2000)]\n",
    "\n",
    "dhelp.plot(bins=50, kind='hist', alpha=0.7)\n",
    "dunhelp.plot(bins=50, kind='hist',alpha=0.7)\n",
    "\n",
    "plt.legend(['Overall', 'Helpfulness', 'Unhelpfulness'])\n",
    "plt.xlabel(\"length of text\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of text length for helpfulness and unhelpfulness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering: rating_class. Rating class can help us see trends in reviews over time. We will have to calculate it based on star reviews. This feature will be used to examine the good ratings year after year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(star_rating):\n",
    "    if (star_rating == 5) or (star_rating == 4):\n",
    "        return \"Positive\"\n",
    "    elif star_rating == 3:\n",
    "        return \"Neutral\"\n",
    "    elif (star_rating == 2) or (star_rating == 1):\n",
    "        return \"Negative\"\n",
    "# Add sentiments to the data\n",
    "el[\"rating_class\"] = el[\"star_rating\"].apply(sentiments)\n",
    "el[\"rating_class\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the rating classes change for each year in a table and graph.\n",
    "# Create a dataframe which shows the relationship between review numbers and rating classes for each year\n",
    "\n",
    "el['rating_class_num'] = el['rating_class'].map({'Positive': 1, 'Negative': 0})\n",
    "    \n",
    "feature = el.groupby('year').agg({'rating_class_num':['size', 'sum', 'mean']})\n",
    "feature.columns = [' '.join(col).strip() for col in feature.columns.values]\n",
    "feature = feature.reset_index()\n",
    "feature['rating_class_num mean'] = feature['rating_class_num mean']*100\n",
    "feature.columns = ['Year', 'Total Reviews', 'Helpful Votes', \n",
    "                       '% of Helpful Reviews in This Year'] \n",
    "feature['% of Helpful Reviews in This Year'] = (el[\"helpful_votes\"] / 317386)*100\n",
    "        \n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph for each rating class to compare within the years\n",
    "##################################################################\n",
    "\n",
    "el['rating_class_num'] = el['rating_class'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "feature = el.groupby('year').agg({'rating_class_num':['size', 'sum', 'mean']})\n",
    "feature.columns = [' '.join(col).strip() for col in feature.columns.values]\n",
    "feature = feature.reset_index()\n",
    "feature['rating_class_num mean'] = feature['rating_class_num mean']*100\n",
    "feature.columns = ['Year', 'Total Reviews', 'Helpful Votes', \n",
    "                       '% of Helpful Reviews in This Year'] \n",
    "plt.figure(figsize = (12,8))\n",
    "fig0, ax1 = plt.subplots(figsize = (12,6))\n",
    "ax2 = ax1.twinx()\n",
    "feature.set_index(feature['Year'], drop=True, inplace=True)\n",
    "feature[\"Total Reviews\"].plot(kind = 'bar', stacked = True, ax = ax1, colormap = 'summer', rot=0)\n",
    "feature['% of Helpful Reviews in This Year'].plot(use_index = False, \n",
    "                                                         kind = 'line', ax = ax2, colormap='Spectral')   \n",
    "\n",
    "plt.title('Good Ratings Compared to the Each Year', color='r')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.yaxis.label.set_color('blue')\n",
    "ax1.set_ylabel(\"Total Reviews\")\n",
    "ax2.set_ylabel(\"Good Rating Percentage for Each Year\")\n",
    "ax2.yaxis.label.set_color('red')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the early 2000's, people were still reliant on hard retail stores. This changed as the internet became a source of retail, which explains how in 2000 there was only 1 review and as time progressed, more people were giving feedback on their experiences of online retail. We see that from 2009-2015, good rating review percentages appear to stabilize around 85-86% of all reviews given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to understand which numeric attributes are correlated with one another. A cross correlation heatmap was generated to provide insight as to which numeric attributes are highly correlated to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation matrix between numeric variables\n",
    "plt.figure(figsize = (14,14))\n",
    "sns.heatmap(el.corr(method=\"pearson\"), cmap='Blues', annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn allows you to make plots of each numeric attribute to get a sense of the correlation holisticly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to computational considerations, I will reduce reduce the number of observations. I will drop good rating class reviews longer than 250 words, and I will drop all observations earlier than year 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el['review_length'] = el.cleaned_review_bodies.apply(len)\n",
    "\n",
    "#drop reviews > 250 words\n",
    "el1 = el.drop(el[(el['review_length'] > 250) & (el['rating_class'] == 'good')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el2 = el1.drop(el1[(el1['year'] < str(2010))].index)\n",
    "el2=el2.head(15000)\n",
    "el2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el2['review_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with clean text and rating class number\n",
    "el3 = el2[[\"cleaned_review_bodies\", \"rating_class_num\"]].reset_index()\n",
    "el3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the countervectorizer\n",
    "countVec = CountVectorizer(ngram_range=(1, 2),binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the 'clean_text' to countvectorizer\n",
    "countVec.fit(el3[\"cleaned_review_bodies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the matriz\n",
    "transformed_matrix = countVec.transform(el3[\"cleaned_review_bodies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matrix to array\n",
    "transformed_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature names\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a 'rating' column from previous dataframe's rating value\n",
    "el4 = pd.DataFrame(transformed_matrix.toarray(), columns=names)\n",
    "el4['rating'] = el3['rating_class_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el4['rating'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for forming a dataframe summary\n",
    "feature_names = []\n",
    "avg_ratings = [] \n",
    "rating_counts = []\n",
    "for name in names:\n",
    "    if name != 'rating':    \n",
    "        avg_rating = el4[el4[name]== 1]['rating'].mean()\n",
    "        rating_count = el4[el4[name]== 1]['rating'].count()\n",
    "        feature_names.append(name)\n",
    "        avg_ratings.append(avg_rating)\n",
    "        rating_counts.append(rating_count)  \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create a new dataframe from words, average ratings, and rating counts\n",
    "el_summary = pd.DataFrame({'feature_name':feature_names, 'avg_rating': avg_ratings, 'rating_count':rating_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the new dataframe\n",
    "el_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that are commonly used in the reviews which have good ratings\n",
    "el_good = el_summary.query(\"rating_count > 20\").sort_values(by='avg_rating', ascending=False)[4:50]\n",
    "el_good.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_good = dict(zip(el_good['feature_name'].tolist(), el_good['avg_rating'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(wc_good)\n",
    " \n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (20, 20), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that are commonly used in the reviews which have bad ratings\n",
    "el_bad = el_summary.query(\"rating_count > 10\").sort_values(by= 'avg_rating', ascending=True)[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_bad.sort_values(by=['avg_rating'],ascending=False,inplace=True)\n",
    "el_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_bad = dict(zip(el_bad['feature_name'].tolist(), df_bad['avg_rating'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate_from_frequencies(wc_bad)\n",
    " \n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (20, 20), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of doing this cleaning, pre-processing, and analytical work will allow me to then perform sentiment analysis and product recommendation in the second portion of this class by using Naive Bayes, Random Forest, Logistic Regression, and some unsupervised learning methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sources and guides for techniques employed for this project. \n",
    "#https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "#https://towardsdatascience.com/sentiment-analysis-and-product-recommendation-on-amazons-electronics-dataset-reviews-part-1-6b340de660c2\n",
    "#https://towardsdatascience.com/sentiment-analysis-on-amazon-reviews-45cd169447ac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
